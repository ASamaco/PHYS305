{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Parallel Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Modern scientific computing tasks often involve massive datasets and computationally expensive algorithms.\n",
    "Problems like large-scale simulations, statistical sampling (e.g., Monte Carlo methods), and real-time data processing demand a level of performance that cannot be achieved with sequential execution alone.\n",
    "\n",
    "Parallel computing is the practice of dividing a problem into smaller subproblems that can be solved simultaneously.\n",
    "With the rise of multicore processors, distributed systems, and GPUs, parallel computing is now essential for high-performance computing (HPC).\n",
    "\n",
    "This lecture introduces key ideas, theoretical limits, and practical tools for parallel computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Theoretical Foundations\n",
    "\n",
    "Before we explore specific tools and implementations, it's important to understand the theoretical limits of parallel computing.\n",
    "These foundational concepts help us answer questions like:\n",
    "* What is the maximum possible speedup if we parallelize a task?\n",
    "* Where should we invest our effort to gain performance?\n",
    "* Why do some problems benefit more from parallelization than others?\n",
    "\n",
    "In addition, scaling analyses provide practical ways to assess real-world performance of parallel codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Amdahl's Law\n",
    "\n",
    "Let $f$ be the fraction of a program that must be executed sequentially.\n",
    "The maximum speed-up $S$ obtainable with $P$ processors is:\n",
    "\\begin{align}\n",
    "  S(P) = \\frac{1}{f + (1-f)/P}.\n",
    "\\end{align}\n",
    "As $P \\to \\infty$, $S \\to 1/f$.\n",
    "\n",
    "**Implication:** Even small sequential portions limit total speedup.\n",
    "\n",
    "Amdahl's law illustrates that optimizing the serial part of a program can be more impactful than parallelizing the rest.\n",
    "For example, if 5% of the computation is inherently sequential, no matter how many processors are used, we cannot speed up the program more than 20x.\n",
    "This is espeically important when one design algorithms for leadership HPC (e.g., DOE Frontier).\n",
    "It corresponds to \"strong scaling tests\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Gustafson's Law\n",
    "\n",
    "Recognizes that in order to fully utilize computing resources, problem size often needs to scale with the number of processors:\n",
    "\\begin{align}\n",
    "  S(P) = P - f(P - 1)\n",
    "\\end{align}\n",
    "Assumes the workload increases with $P$, thus avoiding Amdahl's pessimism.\n",
    "\n",
    "**Implication:** In practice, we often scale up problems as we add more resources.\n",
    "This law gives a more optimistic and realistic view in scientific computing, where we often increase the resolution or domain size with more computing power.\n",
    "It corresponds to \"weak scaling tests\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Flynn's Taxonomy\n",
    "\n",
    "A classification of computer architectures:\n",
    "\n",
    "* SISD: Single Instruction Single Data (standard CPU)\n",
    "* SIMD: Single Instruction Multiple Data (vector processors, GPUs)\n",
    "* MISD: Rare, mostly theoretical\n",
    "* MIMD: Multiple Instruction Multiple Data (clusters, multicore CPUs)\n",
    "\n",
    "Flynn's taxonomy helps us map programming models to the underlying hardware.\n",
    "For instance, OpenMP typically targets MIMD systems with shared memory, while SIMD models underpin GPUs and vectorized CPU instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Monte Carlo Computation of $\\pi$\n",
    "\n",
    "We will parallelize two algorithms using different techniques.\n",
    "The first algorithm is monte carlo computation of $\\pi$.\n",
    "This is an embarrassingly parallel problem.\n",
    "So not much actual algorithm consideration is needed.\n",
    "We main use it to get ourselve familiar with different tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Python Series Code\n",
    "\n",
    "Here is the algorithm in native python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def mcpi_loop(n_total=1000_000):\n",
    "    n_inside = 0\n",
    "    for _ in range(n_total):\n",
    "        x, y = random.random(), random.random()\n",
    "        if x*x + y*y < 1.0:\n",
    "            n_inside += 1\n",
    "\n",
    "    return 4 * n_inside / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = mcpi_loop()\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit mcpi_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "On my laptop it takes about 80ms to perform 1M samples.\n",
    "The number of significant digits is $\\sim \\log_{10}\\sqrt{N} = 3$. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
