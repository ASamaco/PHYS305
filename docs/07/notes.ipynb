{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Numerical and Automatic Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Derivatives are fundamental in mathematical modeling.\n",
    "They provide essential insights into the behavior of physical systems by quantifying rates of change.\n",
    "\n",
    "In fields such as computational physics, engineering, and machine learning, the efficient and accurate computation of derivatives is crucial for simulations, optimizations, and analyses.\n",
    "Traditional analytical methods for finding derivatives may become intractable for complex or nonlinear functions commonly encountered in real-world applications.\n",
    "Consequently, alternative techniques have emerged as indispensable tools in scientific computing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "The derivative of a real-valued function $f(x)$ at a point $x = a$ is defined as the limit:\n",
    "\\begin{align}\n",
    "f'(a) = \\lim_{h \\to 0} \\frac{f(a + h) - f(a)}{h}.\n",
    "\\end{align}\n",
    "This limit, if it exists, represents the slope of the tangent line to the curve $y = f(x)$ at $x = a$. The derivative function $f'(x)$ provides the rate of change of $f$ at any point within its domain where the derivative exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Several fundamental rules facilitating the computation of derivatives are taught in undergraduate calculus courses.\n",
    "Among them, the most important one is the chain rule.\n",
    "It states that, for $f(x) = g(h(x))$, its derivative is given by\n",
    "\\begin{align}\n",
    "f'(x) = g'(h(x)) h'(x).\n",
    "\\end{align}\n",
    "We will show that the chain rule is extremely important in modern numerical and automatic derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Methods for computing derivatives include symbolic differentiation, numerical approximation, and automatic differentiation.\n",
    "Symbolic differentiation applies analytical rules directly to mathematical expressions, yielding exact derivative formulas.\n",
    "Numerical methods, such as finite difference techniques, approximate derivatives using discrete data points and are straightforward to implement but may suffer from truncation and round-off errors.\n",
    "Automatic differentiation bridges the gap by systematically applying the chain rule to compute exact derivatives up to machine precision without symbolic manipulation, making it efficient for complex functions and large-scale systems.\n",
    "\n",
    "Understanding the principles, advantages, and limitations of these approaches allows for the selection of the most appropriate method for a given problem.\n",
    "This lecture will introduce these techniques, providing a comprehensive overview of their theoretical foundations and practical implementations in computational contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Symbolic Differentiation\n",
    "\n",
    "Symbolic differentiation computes the derivative of a function expressed symbolically by applying calculus rules directly to its mathematical expression.\n",
    "Unlike numerical methods that approximate derivatives at specific points, symbolic differentiation yields exact analytical expressions, making it valuable for theoretical analyses and precise computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Algorithmic Approach\n",
    "\n",
    "The general algorithm for symbolic differentiation involves:\n",
    "\n",
    "1. Parsing the Expression: Represent the function as an expression tree, where each node corresponds to an operation (e.g., addition, multiplication) or operand (e.g., variable, constant).\n",
    "2. Applying Differentiation Rules: Recursively apply differentiation rules to each node in the expression tree, including the chain rule.\n",
    "3. Simplifying the Result: After applying the differentiation rules, simplify the resulting expression to make it more readable and computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Consider the function $f(x) = x^2 \\sin(x) + e^{2x}$.\n",
    "To compute $f'(x)$, a symbolic differentiation system would:\n",
    "1. Differentiate $x^2 \\sin(x)$ using the product rule:\n",
    "   \\begin{align}\n",
    "   \\frac{d}{dx}[x^2 \\sin(x)] = x^2 \\cos(x) + 2 x \\sin(x)\n",
    "   \\end{align}\n",
    "2. Differentiate $e^{2x}$ using the chain rule:\n",
    "   \\begin{align}\n",
    "   \\frac{d}{dx}[e^{2x}] = 2 e^{2x}\n",
    "   \\end{align}\n",
    "3. Combine the results:\n",
    "   \\begin{align}\n",
    "   f'(x) = x^2 \\cos(x) + 2 x \\sin(x) + 2 e^{2x}\n",
    "   \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Symbolic Computation with SymPy\n",
    "\n",
    "`SymPy` is an open-source Python library for symbolic mathematics.\n",
    "It allows for symbolic differentiation and manipulation of mathematical expressions.\n",
    "\n",
    "Using `SymPy` to Compute $f'(x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "x = sp.symbols('x')\n",
    "f = x**2 * sp.sin(x) + sp.exp(2 * x)\n",
    "f_prime = sp.diff(f, x)\n",
    "f_prime_simplified = sp.simplify(f_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_prime_simplified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Advantages of Symbolic Differentiation\n",
    "\n",
    "Symbolic differentiation provides exact results by producing precise analytical expressions without approximation errors.\n",
    "This exactness is crucial in theoretical work where precise solutions are necessary.\n",
    "Additionally, symbolic derivatives are valid over continuous ranges of variables, offering general applicability in analyses.\n",
    "They facilitate further analytical processes by simplifying tasks such as solving differential equations and optimizing functions, as the exact expressions can be manipulated algebraically to gain deeper insights into the behavior of the system under study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "Symbolic differentiation has limitations that can impact its practicality.\n",
    "One challenge is the potential for expression growth of the derivative expressions as the original function's complexity increases.\n",
    "This growth can make the expressions difficult to interpret and computationally intensive to process.\n",
    "The computational complexity associated with differentiating complex functions can require substantial resources and time, especially for high-dimensional systems or functions involving intricate compositions.\n",
    "Furthermore, symbolic differentiation is not suitable for functions without explicit symbolic forms, such as those defined by experimental data, simulations, or complex numerical algorithms.\n",
    "In such cases, alternative methods like numerical differentiation or automatic differentiation are more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Software Tools\n",
    "\n",
    "Several software tools facilitate symbolic differentiation by automating the application of calculus rules to mathematical expressions:\n",
    "\n",
    "* [`SymPy`](https://www.sympy.org/):\n",
    "  An open-source Python library that provides capabilities for symbolic differentiation, integration, and equation solving within the Python ecosystem.\n",
    "* [`Mathematica`](https://www.wolfram.com/mathematica/):\n",
    "  A computational software developed by Wolfram Research, offering extensive symbolic computation features used widely in academia and industry.\n",
    "* [`Maple`](https://www.maplesoft.com/):\n",
    "  A software package designed for symbolic and numeric computing, providing powerful tools for mathematical analysis.\n",
    "* [`Maxima`](https://maxima.sourceforge.io/):\n",
    "  An open-source computer algebra system specializing in symbolic manipulation, accessible for users seeking free alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Numerical Differentiation\n",
    "\n",
    "Numerical differentiation estimates the derivative of a function using discrete data points, providing approximate values where analytical derivatives are difficult or impossible to obtain.\n",
    "Unlike symbolic differentiation, which yields exact expressions, numerical methods offer flexibility in handling complex, empirical, or high-dimensional functions by leveraging computational algorithms to approximate derivatives at specific points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Finite Difference Methods\n",
    "\n",
    "Finite difference methods are fundamental techniques in numerical differentiation, estimating derivatives by evaluating the function at specific points and computing the ratio of differences.\n",
    "These methods are essential when analytical derivatives are difficult or impossible to obtain, particularly for complex or empirical functions encountered in scientific and engineering applications.\n",
    "\n",
    "The core idea behind finite difference methods is to approximate the derivative $f'(x)$ by evaluating the function $f(x)$ at selected points around $x$.\n",
    "The three most basic finite difference approximations are forward difference, backward difference, and central difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "The **forward difference approximation** estimates the first derivative at a point $x$ by using the function values at $x$ and $x + h$, where $h$ is a small step size:\n",
    "\\begin{align}\n",
    "f'(x) \\approx \\frac{f(x + h) - f(x)}{h}.\n",
    "\\end{align}\n",
    "This method is straightforward to implement and requires only one additional function evaluation beyond the point of interest.\n",
    "However, its accuracy is limited by a truncation error of order $\\mathcal{O}(h)$.\n",
    "As $h$ decreases, the approximation becomes more accurate, but excessively small values of $h$ can lead to significant round-off errors due to the limitations of floating-point arithmetic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Similarly, the **backward difference approximation** estimates the derivative using the function values at $x$ and $x - h$:\n",
    "\\begin{align}\n",
    "f'(x) \\approx \\frac{f(x) - f(x - h)}{h}.\n",
    "\\end{align}\n",
    "Like the forward difference, the backward difference method has the same truncation error of $\\mathcal{O}(h)$.\n",
    "It is particularly useful in situations where function evaluations at points greater than $x$ are not available or are computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "The **central difference approximation** provides a more accurate estimate by averaging the forward and backward differences:\n",
    "\\begin{align}\n",
    "f'(x) \\approx \\frac{1}{2}\\left[\\frac{f(x + h) - f(x)}{h} + \\frac{f(x) - f(x - h)}{h}\\right] = \\frac{f(x + h) - f(x - h)}{2h}.\n",
    "\\end{align}\n",
    "This method reduces the truncation error to $\\mathcal{O}(h^2)$, making it  more accurate for smooth functions.\n",
    "The central difference requires function evaluations at both $x + h$ and $x - h$, effectively doubling the number of required computations compared to the forward or backward methods.\n",
    "Nevertheless, the enhanced accuracy often justifies the additional computational effort, especially in applications demanding high precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Finite difference methods involve a trade-off between truncation error and round-off error.\n",
    "The truncation error arises from approximating the derivative using discrete differences, while the round-off error is due to the finite precision of floating-point arithmetic used in computations.\n",
    "\n",
    "For the forward and backward difference methods, the truncation error is proportional to $h$, meaning that decreasing $h$ improves the approximation's accuracy linearly.\n",
    "In contrast, the central difference method's truncation error decreases quadratically with $h$, offering better accuracy for smaller step sizes.\n",
    "\n",
    "However, reducing $h$ too much can amplify round-off errors, as the difference $f(x + h) - f(x)$ becomes dominated by floating-point precision limitations.\n",
    "Therefore, selecting an optimal step size $h$ is crucial.\n",
    "Typically, $h$ is chosen to balance the minimization of both truncation and round-off errors, often on the order of the square root of [machine epsilon](sec:machine-accuracy) (e.g., $h \\approx \\sqrt{\\epsilon}$), where machine epsilon represents the smallest difference recognizable by the floating-point system.m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Sample Implementation\n",
    "\n",
    "To illustrate finite difference methods, consider the following Python implementation of the forward, backward, and central difference approximations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fx_forward(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "def fx_backward(f, x, h):\n",
    "    return (f(x) - f(x - h)) / h\n",
    "\n",
    "def fx_central(f, x, h):\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X    = np.linspace(0, 2 * np.pi, 32)\n",
    "f    = lambda x: np.sin(x)\n",
    "fx   = lambda x: np.cos(x)\n",
    "fx_f = lambda x: fx_forward(np.sin, x, X[1])\n",
    "\n",
    "fig, axes = plt.subplots(2,1, figsize=(8, 4))\n",
    "\n",
    "axes[0].plot(X, f(X),    'o-')\n",
    "axes[1].plot(X, fx(X),   '-',  label='Analytical')\n",
    "axes[1].plot(X, fx_f(X), '--', label='Forward Difference')\n",
    "axes[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errs(x0):\n",
    "    fx0 = np.cos(x0) # true derivative\n",
    "    hs  = np.logspace(0, -15, 31) # step sizes\n",
    "    errs_f = [abs(fx_forward (f, x0, h) - fx0) for h in hs]\n",
    "    errs_b = [abs(fx_backward(f, x0, h) - fx0) for h in hs]\n",
    "    errs_c = [abs(fx_central (f, x0, h) - fx0) for h in hs]\n",
    "    return hs, errs_f, errs_b, errs_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, figsize=(12, 4), sharey=True)\n",
    "\n",
    "for i, x0 in enumerate([0, np.pi/4, np.pi/2]):\n",
    "    hs, errs_f, errs_b, errs_c = errs(x0)\n",
    "\n",
    "    axes[i].loglog(hs, hs,    lw=0.5, color='k')\n",
    "    axes[i].loglog(hs, hs**2, lw=0.5, color='k')\n",
    "    axes[i].loglog(hs, hs**4, lw=0.5, color='k')\n",
    "    axes[i].loglog(hs, errs_f, 'o-',  label='Forward Difference')\n",
    "    axes[i].loglog(hs, errs_b, 's--', label='Backward Difference')\n",
    "    axes[i].loglog(hs, errs_c, '^:',  label='Central Difference')\n",
    "    axes[i].set_xlim(1e1, 1e-16)\n",
    "    axes[i].set_ylim(1e-17, 1e0)\n",
    "    axes[i].set_xlabel('Step size h')\n",
    "    axes[i].grid(True, which=\"both\", ls=\"--\")\n",
    "\n",
    "axes[0].set_ylabel('Absolute Error')\n",
    "axes[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Why do the convergence rates do not behave as expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Selecting an appropriate step size $h$ is critical for minimizing the total error in finite difference approximations.\n",
    "An optimal $h$ balances the reduction of truncation error with the increase in round-off error.\n",
    "Empirical testing or theoretical estimates, such as $h \\approx \\sqrt{\\epsilon}$, where $\\epsilon$ is the machine epsilon, can guide the selection of $h$.\n",
    "In practice, $h$ is often chosen through experimentation to achieve the best trade-off for the specific function and computational environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### High-Order Finite Difference Methods: Finite Difference Method of High-Order Derivatives\n",
    "\n",
    "Finite difference methods can also be extended beyond first derivatives to approximate higher-order derivatives with increased accuracy.\n",
    "One common approach to obtain a second derivative approximation involves combining two central difference formulas.\n",
    "\n",
    "To derive the finite difference approximation for the second derivative, consider the Taylor series expansions of $f(x + h)$ and $f(x - h)$ around the point $x$:\n",
    "\\begin{align}\n",
    "f(x + h) &= f(x) + h f'(x) + \\frac{h^2}{2} f''(x) + \\frac{h^3}{6} f'''(x) + \\mathcal{O}(h^4), \\\\\n",
    "f(x - h) &= f(x) - h f'(x) + \\frac{h^2}{2} f''(x) - \\frac{h^3}{6} f'''(x) + \\mathcal{O}(h^4).\n",
    "\\end{align}\n",
    "Adding these two equations eliminates the first and third derivative terms:\n",
    "\\begin{align}\n",
    "f(x + h) + f(x - h) = 2 f(x) + h^2 f''(x) + \\mathcal{O}(h^4).\n",
    "\\end{align}\n",
    "Rearranging the equation to solve for $f''(x)$:\n",
    "\\begin{align}\n",
    "f''(x) \\approx \\frac{f(x + h) - 2 f(x) + f(x - h)}{h^2} + \\mathcal{O}(h^2).\n",
    "\\end{align}\n",
    "This yields the central difference formula for the second derivative with a truncation error of order $\\mathcal{O}(h^2)$.\n",
    "It requires evaluating the function at three points: $x - h$, $x$, and $x + h$.\n",
    "\n",
    "To demonstrate the application of the central difference formula for the second derivative, consider the function $f(x) = \\sin(x)$.\n",
    "The true second derivative of this function is $f''(x) = -\\sin(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fxx_central(f, x, h):\n",
    "    return (f(x + h) - 2 * f(x) + f(x - h)) / h**2\n",
    "\n",
    "def errs2(x0):\n",
    "    fxx0 = -np.sin(x0) # true derivative\n",
    "    hs   = np.logspace(0, -15, 31) # step sizes\n",
    "    errs = [abs(fxx_central(f, x0, h) - fxx0) for h in hs]\n",
    "    return hs, errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, figsize=(12, 4), sharey=True)\n",
    "\n",
    "for i, x0 in enumerate([0, np.pi/4, np.pi/2]):\n",
    "    hs, errs2_c = errs2(x0)\n",
    "    axes[i].loglog(hs, hs,    lw=0.5, color='k')\n",
    "    axes[i].loglog(hs, hs**2, lw=0.5, color='k')\n",
    "    axes[i].loglog(hs, hs**4, lw=0.5, color='k')\n",
    "    axes[i].loglog(hs, errs2_c, 'o-', label='Central Difference')\n",
    "    axes[i].set_xlim(1e1, 1e-16)\n",
    "    axes[i].set_ylim(1e-10, 1e15)\n",
    "    axes[i].set_xlabel('Step size h')\n",
    "    axes[i].grid(True, which=\"both\", ls=\"--\")\n",
    "\n",
    "axes[0].set_ylabel('Absolute Error')\n",
    "axes[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Why do the convergence rates do not behave as expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### High-Order Finite Difference Methods: High-Order Scheme for Low-Order Derivatives\n",
    "\n",
    "Finite difference methods can also be extended to achieve higher-order accuracy by incorporating more function evaluation points and carefully constructing linear combinations that cancel out lower-order error terms.\n",
    "High-order finite difference schemes provide more precise derivative approximations, which are particularly beneficial in applications requiring high accuracy, such as computational fluid dynamics, structural analysis, and numerical simulations of physical systems.\n",
    "This is not be confused, however, with previous section, where (low-order) finite difference methods are used to compute high-order derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "To derive high-order finite difference approximations, the standard method is to use Taylor series expansion of the function around the point of interest.\n",
    "By considering multiple points symmetrically distributed around the target point, it is possible to eliminate lower-order error terms, thereby increasing the accuracy of the derivative approximation.\n",
    "\n",
    "Specifically, consider approximating the first derivative $f'(x)$ with fourth-order accuracy.\n",
    "This requires that the truncation error be of order $\\mathcal{O}(h^4)$, meaning that the error decreases proportionally to $h^4$ as the step size $h$ approaches zero.\n",
    "\n",
    "Expand the function $f$ at points $x - 2h$, $x - h$, $x + h$, and $x + 2h$ using the Taylor series around $x$:\n",
    "\\begin{align}\n",
    "f(x - 2h) &= f(x) - 2h f'(x) + \\frac{(2h)^2}{2} f''(x) - \\frac{(2h)^3}{6} f'''(x) + \\frac{(2h)^4}{24} f''''(x) + \\mathcal{O}(h^5), \\\\\n",
    "f(x -  h) &= f(x) -  h f'(x) + \\frac{  h ^2}{2} f''(x) - \\frac{  h ^3}{6} f'''(x) + \\frac{  h ^4}{24} f''''(x) + \\mathcal{O}(h^5), \\\\\n",
    "f(x +  h) &= f(x) +  h f'(x) + \\frac{  h ^2}{2} f''(x) + \\frac{  h ^3}{6} f'''(x) + \\frac{  h ^4}{24} f''''(x) + \\mathcal{O}(h^5), \\\\\n",
    "f(x + 2h) &= f(x) + 2h f'(x) + \\frac{(2h)^2}{2} f''(x) + \\frac{(2h)^3}{6} f'''(x) + \\frac{(2h)^4}{24} f''''(x) + \\mathcal{O}(h^5).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "We will construct linear combinations of these expansions to eliminate the lower-order terms up to $h^3$.\n",
    "For example, subtract the expansion at $x - 2h$ from that at $x + 2h$ and adjust coefficients to isolate $f'(x)$:\n",
    "\\begin{align}\n",
    "f(x + 2h) - f(x - 2h) &= 4h f'(x) + \\frac{8h^3}{3} f'''(x) + \\mathcal{O}(h^5), \\\\\n",
    "f(x +  h) - f(x -  h) &= 2h f'(x) + \\frac{h^3}{3} f'''(x) + \\mathcal{O}(h^5).\n",
    "\\end{align}\n",
    "It is now straightforward to eliminate the $f'''(x)$ term:\n",
    "\\begin{align}\n",
    "-f(x + 2h) + f(x - 2h) + 8f(x + h) - 8f(x - h) = 12h f'(x)  + \\mathcal{O}(h^5).\n",
    "\\end{align}\n",
    "Solving for $f'(x)$:\n",
    "\\begin{align}\n",
    "f'(x) \\approx \\frac{-f(x + 2h) + 8f(x + h) - 8f(x - h) + f(x - 2h)}{12h} + \\mathcal{O}(h^4).\n",
    "\\end{align}\n",
    "This leads to the fourth-order central difference formula for the first derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fx_central4(f, x, h):\n",
    "    return (-f(x+2*h) + 8*f(x+h) - 8*f(x-h) + f(x-2*h))/(12*h)\n",
    "\n",
    "def errs4(x0):\n",
    "    fx0 = np.cos(x0) # true derivative\n",
    "    hs  = np.logspace(0, -15, 31) # step sizes\n",
    "    errs_c4 = [abs(fx_central4(f, x0, h) - fx0) for h in hs]\n",
    "    return hs, errs_c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, figsize=(12, 4), sharey=True)\n",
    "\n",
    "for i, x0 in enumerate([0, np.pi/4, np.pi/2]):\n",
    "    hs, errs_f, errs_b, errs_c = errs(x0)\n",
    "    hs, errs_c4                = errs4(x0)\n",
    "    axes[i].loglog(hs, hs,    lw=0.5, color='k')\n",
    "    axes[i].loglog(hs, hs**2, lw=0.5, color='k')\n",
    "    axes[i].loglog(hs, hs**4, lw=0.5, color='k')\n",
    "    axes[i].loglog(hs, errs_f,  'o-',  label='Forward Difference')\n",
    "    axes[i].loglog(hs, errs_b,  's--', label='Backward Difference')\n",
    "    axes[i].loglog(hs, errs_c,  '^:',  label='Central Difference')\n",
    "    axes[i].loglog(hs, errs_c4, '.--', label='4th-order Central Difference')\n",
    "    axes[i].set_xlim(1e1, 1e-16)\n",
    "    axes[i].set_ylim(1e-17, 1e0)\n",
    "    axes[i].set_xlabel('Step size h')\n",
    "    axes[i].grid(True, which=\"both\", ls=\"--\")\n",
    "\n",
    "axes[0].set_ylabel('Absolute Error')\n",
    "axes[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Does the convergence rate of the 4th-order central difference behave as expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Coefficients of high-order fininte difference for computing low order derivatives have been worked out.\n",
    "One may implementing them simply by following [tables](https://en.wikipedia.org/wiki/Finite_difference_coefficient).\n",
    "Nevertheless, it is unusal to go above 6th-order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Spectral Derivatives\n",
    "\n",
    "Spectral methods are a class of high-accuracy numerical techniques used to approximate derivatives by representing functions in terms of global basis functions.\n",
    "Unlike finite difference methods, which rely on local information, spectral methods utilize the entire domain's information, enabling exponential convergence rates for sufficiently smooth functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Fourier spectral methods leverage the Fourier series representation of periodic functions to compute derivatives with high precision.\n",
    "The fundamental idea is to transform the function into the frequency domain, where differentiation becomes a straightforward algebraic operation.\n",
    "\n",
    "For a function  f(x)  defined on a periodic interval  [L/2, L/2), the Fourier series expansion is given by:\n",
    "\\begin{align}\n",
    "f(x) = \\sum_{n=-N/2}^{N/2} F_n e^{i 2\\pi n x / L} \\equiv \\sum_{n=-N/2}^{N/2} F_k e^{i k_n x},\n",
    "\\end{align}\n",
    "where $N$ is the number of modes, $F_n$ are the Fourier coefficients, and $k_n \\equiv 2\\pi n/L$ are the wavenumbers.\n",
    "\n",
    "Differentiation of $f(x)$ in the spatial domain corresponds to multiplication by $i k_n$ in the frequency domain.\n",
    "Specifically, the first derivative $f'(x$) is:\n",
    "\\begin{align}\n",
    "f'(x) = \\sum_{k=-N/2}^{N/2} i k_n F_k e^{i k_n x}.\n",
    "\\end{align}\n",
    "This property simplifies the computation of derivatives, as it converts the differentiation process into a simple multiplication operation in the Fourier space.\n",
    "\n",
    "Consider computing the first derivative of $f(x) = \\sin(x)$ on the domain $[-\\pi, \\pi)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of modes\n",
    "N = 64\n",
    "\n",
    "# Domain\n",
    "L = 2 * np.pi\n",
    "x = np.linspace(-L/2, L/2, N, endpoint=False)\n",
    "\n",
    "def fx_spectral(func, x):\n",
    "    f  = func(x)\n",
    "    F  = np.fft.fft(f)\n",
    "    k  = 2 * np.pi * np.fft.fftfreq(len(x), d=x[1]-x[0])\n",
    "\n",
    "    # Multiply by ik to get derivative in frequency domain\n",
    "    Fx = 1j * k * F\n",
    "\n",
    "    # Inverse Fourier transform to get derivative in spatial domain\n",
    "    return np.fft.ifft(Fx).real\n",
    "\n",
    "# Spectral derivative\n",
    "fx  = fx_spectral(np.sin, x)\n",
    "\n",
    "# True derivative\n",
    "fx0 = np.cos(x)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, fx,  'ro--', label='Spectral Derivative')\n",
    "plt.plot(x, fx0, 'b.-',  label='True Derivative')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"f'(x)\")\n",
    "plt.title('Fourier Spectral Derivative of sin(2πx)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns     = 2**np.arange(1,10)\n",
    "errs_s = []\n",
    "for N in Ns:\n",
    "    x   = np.linspace(-L/2, L/2, N, endpoint=False)\n",
    "    fx  = fx_spectral(np.sin, x)\n",
    "    fx0 = np.cos(x)\n",
    "    dfx = fx - fx0\n",
    "    err = np.sqrt(np.mean(dfx*dfx))\n",
    "    errs_s.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(Ns, errs_s, 'o-')\n",
    "plt.xlabel('Number of samples N')\n",
    "plt.ylabel('RMS error')\n",
    "plt.grid(True, which=\"both\", ls=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "* How should we interpret this convergence plot?\n",
    "* What if we change the function to, e.g., a Gaussian?\n",
    "* How about Lorentzian $1/(1+x^2)$?\n",
    "* Does the domain matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Complex Step Differentiation\n",
    "\n",
    "The Complex Step Method provides an innovative approach to numerical differentiation by evaluating the function at a complex-valued point.\n",
    "Unlike finite difference methods, which approximate derivatives using real-valued perturbations and suffer from subtractive cancellation errors, the Complex Step Method utilizes an imaginary perturbation to extract derivative information directly from the imaginary component of the function's output.\n",
    "This technique achieves high accuracy without the need for extremely small step sizes, thereby avoiding the numerical instability associated with finite differences.\n",
    "In astrophysics, this method was first used in numerical relativity to mitigate the stability of solving Einstein's field equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "The foundation of the Complex Step Method lies in the application of complex analysis to differentiation.\n",
    "Consider a real-valued function $f(x)$ that is analytic (i.e., differentiable) in a neighborhood around a point $x$.\n",
    "By introducing a small imaginary perturbation $ih$ to the input, where $h$ is a real step size and $i$ is the imaginary unit, we can express the function as:\n",
    "\\begin{align}\n",
    "f(x + ih) = f(x) + ih f'(x) - \\frac{h^2}{2} f''(x) - i \\frac{h^3}{6} f''(x) + \\cdots\n",
    "\\end{align}\n",
    "The key insight is that the imaginary part of $f(x + ih)$ is directly proportional to the first derivative $f'(x)$:\n",
    "\\begin{align}\n",
    "\\operatorname{Im}[f(x + ih)] = h f'(x) + \\mathcal{O}(h^3)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "By isolating the imaginary component and dividing by $h$, we obtain an approximation for the first derivative:\n",
    "\\begin{align}\n",
    "f'(x) \\approx \\frac{\\operatorname{Im}[f(x + ih)]}{h}\n",
    "\\end{align}\n",
    "\n",
    "Subtractive cancellation errors arise in finite difference approximations when two nearly equal real numbers are subtracted, leading to a significant loss of precision.\n",
    "For example, in the central difference formula:\n",
    "\\begin{align}\n",
    "f'(x) \\approx \\frac{f(x + h) - f(x - h)}{2h}\n",
    "\\end{align}\n",
    "if $h$ is very small, $f(x + h)$ and $f(x - h)$ become nearly identical, and their difference is subject to floating-point round-off errors. This limits the accuracy of the derivative approximation.\n",
    "\n",
    "In contrast, the Complex Step Method avoids this issue by perturbing the input in the imaginary direction.\n",
    "The derivative information is captured in the imaginary part, which is not subject to the same precision loss.\n",
    "As a result, the Complex Step Method can achieve derivative approximations accurate to machine precision without requiring excessively small step sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fx_complexstep(f, x, h):\n",
    "    return np.imag(f(x + 1j * h)) / h\n",
    "\n",
    "def errscs(x0):\n",
    "    fx0 = np.cos(x0) # true derivative\n",
    "    hs  = np.logspace(0, -15, 31) # step sizes\n",
    "    errs_c4 = [abs(fx_complexstep(f, x0, h) - fx0) for h in hs]\n",
    "    return hs, errs_c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, figsize=(12, 4), sharey=True)\n",
    "\n",
    "for i, x0 in enumerate([0, np.pi/4, np.pi/2]):\n",
    "    hs, errs_f, errs_b, errs_c = errs(x0)\n",
    "    hs, errs_c4                = errs4(x0)\n",
    "    hs, errs_cs                = errscs(x0)\n",
    "    axes[i].loglog(hs, hs,    lw=0.5, color='k')\n",
    "    axes[i].loglog(hs, hs**2, lw=0.5, color='k')\n",
    "    axes[i].loglog(hs, hs**4, lw=0.5, color='k')\n",
    "    axes[i].loglog(hs, errs_c,  '^:',  label='Central Difference')\n",
    "    axes[i].loglog(hs, errs_c4, '.--', label='4th-order Central Difference')\n",
    "    axes[i].loglog(hs, errs_cs, '.--', label='Complex Step Differentiation')\n",
    "    axes[i].set_xlim(1e1, 1e-16)\n",
    "    axes[i].set_ylim(1e-30, 1e0)\n",
    "    axes[i].set_xlabel('Step size h')\n",
    "    axes[i].grid(True, which=\"both\", ls=\"--\")\n",
    "\n",
    "axes[0].set_ylabel('Absolute Error')\n",
    "axes[2].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "* What is the convergent rate of Complex Step Differentiation?\n",
    "* Is it really better than finite difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
