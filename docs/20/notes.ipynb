{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods III: Parameter Estimation and Markov Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This hands-on is based on [Gravitational Wave Open Data Workshop 2024](https://github.com/gw-odw/odw-2024/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Gravitational waves are ripples in spacetime predicted by Albert Einstein's General Theory of Relativity.\n",
    "These waves propagate outward from sources such as merging black holes, neutron star collisions, or rapidly rotating neutron stars.\n",
    "The first direct detection of gravitational waves, GW150914, was observed by the LIGO detectors in 2015, opening a new window into observing astrophysical phenomena previously inaccessible by electromagnetic observations alone.\n",
    "\n",
    "Detecting and analyzing gravitational waves allows astronomers and physicists to study objects and phenomena that emit very little or no light, providing valuable insights into the behavior of gravity in extreme environments and the properties of dense objects like black holes and neutron stars.\n",
    "\n",
    "In this hands-pon, we introduce Bayesian inference methods essential for estimating the parameters of gravitational wave signals detected by instruments such as LIGO and Virgo.\n",
    "Specifically, we'll demonstrate parameter estimation using both a rejection method and Monte Carlo Markov Chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Bayesian Inference: A Brief Overview\n",
    "\n",
    "Recalling that Bayesian inference allows us to update our knowledge of model parameters based on observed data.\n",
    "Bayes' theorem mathematically represents this process:\n",
    "\\begin{align}\n",
    "  p(\\theta|d,M) = \\frac{\\mathcal{L}(d|\\theta,M) \\pi(\\theta|M)}{\\mathcal{Z}(d|M)}\n",
    "\\end{align}\n",
    "where:\n",
    "* $p(\\theta|d, M)$ is the posterior distribution.\n",
    "* $\\mathcal{L}(d|\\theta, M)$ is the likelihood function, measuring agreement between model predictions and observed data.\n",
    "* $\\pi(\\theta|M)$ is the prior distribution, representing previous knowledge.\n",
    "* $\\mathcal{Z}(d|M)$ is the evidence, normalizing the posterior.\n",
    "\n",
    "Typically, the posterior cannot be computed analytically, requiring computational approximations.\n",
    "Stochastic (Monte Carlo) sampling is a common method for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Toy Model\n",
    "\n",
    "We download sample observational data (`toy_model.csv`) containing simulated time-series observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isfile(\"toy_model.csv\"):\n",
    "    print(\"Downloading toy_model.csv\")\n",
    "    ! wget https://raw.githubusercontent.com/gw-odw/odw-2024/main/Tutorials/Day_3/toy_model.csv\n",
    "else:\n",
    "    print(\"toy_model.csv exists; not downloading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Our example data (`toy_model.csv`) contains measurements (`yobs`) recorded at specific times (`time`).\n",
    "Let's visualize these observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "time, yobs = np.genfromtxt(\"toy_model.csv\", delimiter=\",\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(time, yobs)\n",
    "plt.xlabel(\"Time [s]\")\n",
    "plt.ylabel(\"Observed values\")\n",
    "plt.title(\"Simulated Observational Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "We propose the following sine-Gaussian model to explain the data:\n",
    "\\begin{align}\n",
    "  s(t; f,\\alpha) = e^{-(t/\\alpha)^2} \\sin(2\\pi f t),\n",
    "\\end{align}\n",
    "with frequency parameter $f$ and damping parameter $\\alpha$.\n",
    "To build intuition, we visualize this model for representative parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoid(time, freq):\n",
    "    return np.sin(2 * np.pi * freq * time)\n",
    "\n",
    "def gaussian_exponential(time, alpha):\n",
    "    return np.exp(-(time/alpha)**2)\n",
    "\n",
    "def model_Ms(time, freq, alpha):\n",
    "    return gaussian_exponential(time, alpha) * sinusoid(time, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq, alpha = 2, 0.5\n",
    "\n",
    "plt.plot(time, sinusoid(time, freq), label=\"Sinusoid\")\n",
    "plt.plot(time, gaussian_exponential(time, alpha), label=\"Gaussian envelope\")\n",
    "plt.plot(time, model_Ms(time, freq, alpha), label=\"Sine-Gaussian\")\n",
    "plt.xlabel(\"Time [s]\")\n",
    "plt.ylabel(\"Model components\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Parameter Estimation\n",
    "\n",
    "With the data (`yobs`) and our model $M_s$, we can now estimate the parameters $f$ and $\\alpha$.\n",
    "To do this, similar to previous lectures, we will use Bayes theorem, i.e. we want to approximate the distribution\n",
    "\\begin{align}\n",
    "p(\\theta | d, M_s) = \\frac{\\mathcal{L}(d| \\theta, M_s) \\;\\pi(\\theta | M_s)}{\\mathcal{Z}(d | M_s)}\n",
    "\\end{align}\n",
    "where $\\theta=\\{f, \\alpha\\}$ is the two-dimensional parameter vector and $d$ is `yobs` (measured at times `time`).\n",
    "\n",
    "To this end, we need to define the likelihood and priors.\n",
    "Note that, if we are **only interested in the shape of the distribution**, then we can ignore the evidence, i.e. we can estimate the unnormalized distribution\n",
    "\\begin{align}\n",
    "  p(\\theta | d, M_s) \\propto \\mathcal{L}(d| \\theta, M_s) \\;\\pi(\\theta | M_s)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Likelihood\n",
    "\n",
    "For this toy example, we will assume that the data consists of the generate model $M_s$ and additive white Gaussian noise, i.e.\n",
    "\\begin{align}\n",
    "y_{\\rm obs}(t) = s(t; f, \\alpha) + \\epsilon\n",
    "\\end{align}\n",
    "where $\\epsilon \\sim N(0, \\sigma)$ by which we mean that $\\epsilon$ is drawn from a Gaussian distribution with zero mean and standard deviation $\\sigma=0.1$ (for now, we will assume this is known a priori, but see challenges below for how it could be estimated). \n",
    "\n",
    "This definition of how the data was created allows us to define our likelihood.\n",
    "Namely, given a value of $\\{f, \\alpha\\}$, the likelihood of a single data point $y_i$ (measured at $t_i$) is:\n",
    "\\begin{align}\n",
    "\\mathcal{L}(y_i| f, \\alpha, M_s) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - s(t_i; f, \\alpha))^2}{2\\sigma^2}\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "To extend this to multiple data points, we assume they are independent then\n",
    "\\begin{align}\n",
    "\\mathcal{L}(y_{obs} | f, \\alpha, M_s) = \\prod_i \\mathcal{L}(y_i| f, \\alpha, M_s)\n",
    "\\end{align}\n",
    "In practice, it is wise to work with the logarithm of the likelihood to avoid numerical overflow.\n",
    "Then, we have that\n",
    "\\begin{align}\n",
    "\\log \\mathcal{L}(y_{obs} | f, \\alpha, M_s) = \\sum_{i} -\\frac{1}{2}\\left(\\frac{\\left(y_i - s(t_i; f, \\alpha)\\right)^2}{\\sigma^2} + \\log\\left(2\\pi \\sigma^2\\right)\\right)\n",
    "\\end{align}\n",
    "We now transcribe this into `python`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_Ms(time, yobs, freq, alpha, sigma=0.1):\n",
    "    prediction = model_Ms(time, freq, alpha)\n",
    "    res  = yobs - prediction \n",
    "    logl = -0.5 * (((res/sigma)**2) + np.log(2 * np.pi * sigma**2))\n",
    "    return np.sum(logl, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Priors\n",
    "\n",
    "The second part of Bayes theorem is the *prior*.\n",
    "For our two-component model, we will use a simple disjoint prior (i.e. $\\pi(\\theta | M_s)=\\pi(f| M_s)\\pi(\\alpha | M_s)$) with\n",
    "\\begin{align}\n",
    "  \\pi(f     | M_s) &= \\textrm{Uniform}(0, 5) \\\\\n",
    "  \\pi(\\alpha| M_s) &= \\textrm{Uniform}(0, 1)\n",
    "\\end{align}\n",
    "Let us create a python function to calculate the log of the prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior_Ms(freq, alpha):\n",
    "    \"\"\" Calculate the log prior under the Ms model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    freq: array or float\n",
    "        The frequency at which to calculate the prior\n",
    "    alpha: array or float\n",
    "        The alpha at which to calculate the prior\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    log_prior: array\n",
    "        The log_prior calculated for all freq, alpha samples\n",
    "    \"\"\"\n",
    "    # Convert freq, alpha to numpy arrays\n",
    "    freq = np.atleast_1d(freq)\n",
    "    alpha = np.atleast_1d(alpha)\n",
    "    \n",
    "    # Apply Uniform priors: calculate idxs of array where f, alpha in range\n",
    "    f_min = 0\n",
    "    f_max = 5\n",
    "    f_idxs = (freq > f_min) * (freq < f_max)\n",
    "    \n",
    "    alpha_min = 0\n",
    "    alpha_max = 1\n",
    "    alpha_idxs = (alpha > alpha_min) * (alpha < alpha_max)\n",
    "    \n",
    "    idxs = alpha_idxs * f_idxs\n",
    "    \n",
    "    log_prior_volume = np.log(1/(f_max - f_min) * (1 / (alpha_max - alpha_min)))\n",
    "    \n",
    "    log_prior = np.zeros_like(freq)\n",
    "    log_prior[idxs] = log_prior_volume\n",
    "    log_prior[~idxs] = -np.inf\n",
    "    return log_prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Parameter Estimation: Rejection Sampling\n",
    "\n",
    "Now that we have our likelihood and prior, we will introduce **stochastic (i.e., Monte Carlo) sampling**.\n",
    "We start by using the simplest type of stochastic sampling, rejection sampling.\n",
    "The idea is that to draw samples from a target distribution $p(\\theta | d, M_s)$ which is difficult to sample from, we first generate samples from a generating distribution $g(\\theta$) which is easy to sample from and then weight the samples relative to the target distribution.\n",
    "In practice you can choose any generating distribution you like, but we will use $g(\\theta) = g(f)g(\\alpha)$ where\n",
    "\\begin{align}\n",
    "  g(f)      &= \\textrm{Uniform}(1.8, 2.2) \\\\\n",
    "  g(\\alpha) &= \\textrm{Uniform}(0.2, 0.6)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Our rejection sampling algorithm then proceeds as follows:\n",
    "\n",
    "1. Draw $\\theta'=[f, \\alpha]$ from $g(f)$ and $g(\\alpha)$\n",
    "2. Calculate the probability under the target and generating distributions (i.e. $p(\\theta' | d, M_s)$ and $g(\\theta')$)\n",
    "3. Calculate the weight $w=p(\\theta' | d, M_s) / g(\\theta')$\n",
    "4. Draw a random number $u$ uniformly distributed in $[0, 1]$\n",
    "5. If $w > u$, append $\\theta'$ to a set of samples, otherwise reject it and repeat\n",
    "\n",
    "Continue this loop until an acceptable number of samples have been drawn.\n",
    "The resulting set of samples are then an approximation to $p(\\theta | d, M_s)$ and be used to produce summary statistics or create plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "We now program the algorithm for our test data.\n",
    "However, there are two important differences between this algorithm and the expression above:\n",
    "\n",
    "1. We will work with the unnormalised distribution $p(\\theta | d, M_s)$ (i.e. we don't calculate the evidence $\\mathcal{Z}$).\n",
    "   As a result, $w$ is also unnormalised and so it needs to be normalised before we apply step 5.\n",
    "   Fortunately, we can normalize $w$ once we have a distribution of values.\n",
    "2. For computational efficiency, rather than using a while loop we will instead draw a set of 100000 samples, calculate the weights for each, and then apply rejection sampling.\n",
    "   This utilises numpy array optimization and also enables us to normalise the weights to a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw 10000 samples from g(theta)\n",
    "N = 100000\n",
    "freq_gsamples = np.random.uniform(1.8, 2.2, N)\n",
    "alpha_gsamples = np.random.uniform(0.2, 0.6, N)\n",
    "\n",
    "# Make time a 2D array to enable broadcasting across the samples\n",
    "time_array = time[:, np.newaxis]\n",
    "yobs_array = yobs[:, np.newaxis]\n",
    "\n",
    "# Calculate the log_likelihood and log_prior for all samples\n",
    "log_likelihood_vals = log_likelihood_Ms(time_array, yobs_array, freq_gsamples, alpha_gsamples)\n",
    "log_prior_vals = log_prior_Ms(freq_gsamples, alpha_gsamples)\n",
    "log_posterior_vals = log_likelihood_vals + log_prior_vals\n",
    "\n",
    "# Calculate the weights\n",
    "weights = np.exp(log_posterior_vals)\n",
    "\n",
    "# Normalise the weights\n",
    "weights = weights / max(weights)\n",
    "\n",
    "# Rejection sample\n",
    "keep = weights > np.random.uniform(0, 1, weights.shape)\n",
    "alpha_samples = alpha_gsamples[keep]\n",
    "freq_samples = freq_gsamples[keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "The end result of this is a set of samples `freq_samples` and `alpha_samples` that approximate the posterior distribution.\n",
    "We can get a quick visualisation of these by using the `corner` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corner import corner\n",
    "\n",
    "# Create a corner plot\n",
    "samples = np.array([freq_samples, alpha_samples]).T\n",
    "fig = corner(samples, bins=20, labels=[\"f\", \"alpha\"], show_titles=True, quantiles=[0.16, 0.5, 0.84],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "The plot above shows two 1D histograms (one for each parameter) and one 2D histogram (showing any correlations between the samples).\n",
    "Areas where the posterior is large (i.e. the histogram count is high) represent the most probable values of $f$ and $\\alpha$ which explain the data.\n",
    "\n",
    "The samples can also be used to provide a summary statistic.\n",
    "For example, if you wanted to report the mean and standard deviation interval for $f$, you could do something like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_f = np.mean(freq_samples)\n",
    "std_f = np.std(freq_samples)\n",
    "print(f\"We estimate the the mean and standard deviation of frequency to be {mean_f:0.2f}+/-{std_f:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Typically, in GW astronomy, we use the median and a 90\\% credible interval because the posterior is often non-symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "In this exerise, we have learned that rejection sampling can be used to approximate the posterior distribution.\n",
    "However, we should note that it is highly inefficienct.\n",
    "It works okay here, because we tightly tuned the edges of $g(\\theta)$, but if you go back and increase these to a wider range, you'll see the efficiency quickly drops off.\n",
    "Moreover, the efficiency of rejection sampling also suffers when we start to look at problems in more than 2 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the efficiency\n",
    "efficiency = len(freq_samples) / len(freq_gsamples)\n",
    "print(efficiency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
