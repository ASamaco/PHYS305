{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Optimization methods find values that maximize or minimize an objective function, making them useful across disciplines such as engineering, economics, and data science.\n",
    "Fundamentally, the action principle in physics is an optimization process, where nature selects paths that minimize or extremize an action integral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Gradient Descent Methods\n",
    "\n",
    "Gradient Descent is one of the most widely used optimization techniques, particularly effective for high-dimensional problems in fields such as machine learning.\n",
    "The method iteratively seeks the minimum of a function by taking steps proportional to the negative of its gradient, guiding the search toward lower function values.\n",
    "For differentiable objective functions, gradient descent is fundamental in minimizing errors, making it indispensable for training machine learning models and refining physical models in computational astrophysics.\n",
    "\n",
    "For a function $f(x)$, the gradient $\\nabla f(x)$ points in the direction of steepest ascent.\n",
    "Moving in the opposite direction—along the negative gradient—reduces the function's value. The algorithm updates the parameters iteratively according to:\n",
    "\\begin{align}\n",
    "x_{n+1} = x_n - \\alpha \\nabla f(x_n)\n",
    "\\end{align}\n",
    "where $\\alpha$ is the learning rate, controlling the step size.\n",
    "The choice of $\\alpha$ is critical for convergence: \n",
    "a large $\\alpha$ may cause divergence, where updates overshoot the minimum, while a very small $\\alpha$ can lead to slow convergence, requiring many iterations to make meaningful progress.\n",
    "Proper tuning of $\\alpha$ ensures that the algorithm efficiently converges to a minimum without unnecessary oscillations or divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(df, x, alpha, imax=1000):\n",
    "    for _ in range(imax):\n",
    "        x -= alpha * df(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function and its gradient\n",
    "def f(x):\n",
    "    return (x - 3)**2 + 4\n",
    "\n",
    "def df(x):\n",
    "    return 2 * (x - 3)\n",
    "\n",
    "# Parameters for gradient descent\n",
    "x0    = 0.0  # Starting point for optimization\n",
    "alpha = 0.1\n",
    "\n",
    "# Run gradient descent\n",
    "xmin = gd(df, x0, alpha)\n",
    "print(\"Approximate minimum:\")\n",
    "print(\"  xmin  = \",   xmin )\n",
    "print(\"f(xmin) = \", f(xmin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_hist(df, x, alpha, imax=1000):\n",
    "    X = [x]\n",
    "    for _ in range(imax):\n",
    "        X.append(X[-1] - alpha * df(X[-1]))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X = np.linspace(0, 6, 6001)\n",
    "plt.plot(X, f(X))\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "X = np.array(gd_hist(df, x0, alpha))\n",
    "print(X[-1])\n",
    "\n",
    "plt.plot(X, f(X), '-o')\n",
    "plt.xlim(2.5, 3.5)\n",
    "plt.ylim(3.95,4.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "```{exercise}\n",
    "What will happen if we change the learning rate $\\alpha$?\n",
    "\n",
    "Comment out the plot limits `plt.xlim(2.5, 3.5)` and `plt.ylim(3.95,4.3)` and then try $\\alpha = 0.1$, $0.5$, $0.9$, $1.0$, and $1.1$.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
