{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Optimization methods find values that maximize or minimize an objective function, making them useful across disciplines such as engineering, economics, and data science.\n",
    "Fundamentally, the action principle in physics is an optimization process, where nature selects paths that minimize or extremize an action integral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Gradient Descent Methods\n",
    "\n",
    "Gradient Descent is one of the most widely used optimization techniques, particularly effective for high-dimensional problems in fields such as machine learning.\n",
    "The method iteratively seeks the minimum of a function by taking steps proportional to the negative of its gradient, guiding the search toward lower function values.\n",
    "For differentiable objective functions, gradient descent is fundamental in minimizing errors, making it indispensable for training machine learning models and refining physical models in computational astrophysics.\n",
    "\n",
    "For a function $f(x)$, the gradient $\\nabla f(x)$ points in the direction of steepest ascent.\n",
    "Moving in the opposite direction—along the negative gradient—reduces the function's value. The algorithm updates the parameters iteratively according to:\n",
    "\\begin{align}\n",
    "x_{n+1} = x_n - \\alpha \\nabla f(x_n)\n",
    "\\end{align}\n",
    "where $\\alpha$ is the learning rate, controlling the step size.\n",
    "The choice of $\\alpha$ is critical for convergence: \n",
    "a large $\\alpha$ may cause divergence, where updates overshoot the minimum, while a very small $\\alpha$ can lead to slow convergence, requiring many iterations to make meaningful progress.\n",
    "Proper tuning of $\\alpha$ ensures that the algorithm efficiently converges to a minimum without unnecessary oscillations or divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(df, x, alpha, imax=1000):\n",
    "    for _ in range(imax):\n",
    "        x -= alpha * df(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function and its gradient\n",
    "def f(x):\n",
    "    return (x - 3)**2 + 4\n",
    "\n",
    "def df(x):\n",
    "    return 2 * (x - 3)\n",
    "\n",
    "# Parameters for gradient descent\n",
    "x0    = 0.0  # Starting point for optimization\n",
    "alpha = 0.1\n",
    "\n",
    "# Run gradient descent\n",
    "xmin = gd(df, x0, alpha)\n",
    "print(\"Approximate minimum:\")\n",
    "print(\"  xmin  = \",   xmin )\n",
    "print(\"f(xmin) = \", f(xmin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_hist(df, x, alpha, imax=1000):\n",
    "    X = [x]\n",
    "    for _ in range(imax):\n",
    "        X.append(X[-1] - alpha * df(X[-1]))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X = np.linspace(0, 6, 6001)\n",
    "plt.plot(X, f(X))\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "X = np.array(gd_hist(df, x0, alpha))\n",
    "print(X[-1])\n",
    "\n",
    "plt.plot(X, f(X), '-o')\n",
    "plt.xlim(2.5, 3.5)\n",
    "plt.ylim(3.95,4.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "```{exercise}\n",
    "What will happen if we change the learning rate $\\alpha$?\n",
    "\n",
    "Comment out the plot limits `plt.xlim(2.5, 3.5)` and `plt.ylim(3.95,4.3)` and then try $\\alpha = 0.1$, $0.5$, $0.9$, $1.0$, and $1.1$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Similar to our implementation of Newton-Raphson Method, it is possible to employ `JAX` to automatically obtain the derivative.\n",
    "Here is an updated version of automatic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "\n",
    "def autogd_hist(f, x, alpha, imax=1000):\n",
    "    df = grad(f)\n",
    "    X  = [x]\n",
    "    for _ in range(imax):\n",
    "        X.append(X[-1] - alpha * df(X[-1]))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function and its gradient\n",
    "def f(x):\n",
    "    return (x - 3)**2 + 4\n",
    "\n",
    "# Parameters for gradient descent\n",
    "x0    = 0.0  # Starting point for optimization\n",
    "alpha = 0.9\n",
    "\n",
    "# Run gradient descent\n",
    "Xmin = np.array(autogd_hist(f, x0, alpha))\n",
    "print(\"Approximate minimum:\")\n",
    "print(\"  xmin  = \",   Xmin[-1] )\n",
    "print(\"f(xmin) = \", f(Xmin[-1]))\n",
    "\n",
    "X = np.linspace(0, 6, 6001)\n",
    "plt.plot(X,    f(X))\n",
    "plt.plot(Xmin, f(Xmin), '-o')\n",
    "plt.xlim(2.5, 3.5)\n",
    "plt.ylim(3.95,4.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Gradient Descent with JAX for Multiple Dimensions\n",
    "\n",
    "Multidimensional gradient descent is essential for optimizing functions with multiple parameters, making it the backend of applications such as model fitting and deep learning.\n",
    "\n",
    "In astrophysics, gradient descent refines models by iteratively adjusting parameters to minimize discrepancies between observed data and theoretical predictions.\n",
    "For example, in galaxy modeling, each parameter may correspond to a physical property—such as brightness, size, or position—and gradient descent enables efficient optimization to achieve the best fit to observational data.\n",
    "\n",
    "In deep learning, multidimensional gradient descent is fundamental, as modern neural networks can have millions of parameters.\n",
    "During training, the algorithm minimizes a loss function that quantifies the difference between the model’s predictions and actual outcomes.\n",
    "Automatic differentiation with JAX streamlines gradient calculations, allowing practitioners to train complex models without manually computing derivatives.\n",
    "This capability is particularly valuable for architectures such as convolutional and recurrent neural networks, where gradients must be computed across vast numbers of interconnected parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "The following example demonstrates how to use JAX to perform gradient descent on a multivariable function\n",
    "\\begin{align}\n",
    "f(x, y) = (x - 3)^2 + (y + 4)^2,\n",
    "\\end{align}\n",
    "where the minimum is at $(x, y) = (3, -4)$.\n",
    "By tracking each update step, we can visualize the optimization path as it approaches the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "from jax import jit\n",
    "\n",
    "# Function to perform gradient descent with history tracking\n",
    "def autogd_hist(f, X, alpha, imax):\n",
    "    df = jit(grad(f))  # Use JAX to compute gradient\n",
    "    Xs = [np.array(X)]\n",
    "    for _ in range(imax):\n",
    "        Xs.append(Xs[-1] - alpha * df(Xs[-1]))  # Gradient descent update\n",
    "    return jnp.array(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a multivariable function\n",
    "def f(X):\n",
    "    x, y = X\n",
    "    return (x - 3)**2 + 2 * (y + 4)**2\n",
    "\n",
    "# Parameters for gradient descent\n",
    "X0    = jnp.array([0.0, 0.0]) # Starting point for optimization\n",
    "alpha = 0.1                   # Learning rate\n",
    "imax  = 100                   # Number of iterations\n",
    "\n",
    "# Run gradient descent with history tracking\n",
    "Xs = autogd_hist(f, X0, alpha, imax)\n",
    "print(\"Approximate minimum:\")\n",
    "print(\"  xmin  =\",   Xs[-1] )\n",
    "print(\"f(xmin) =\", f(Xs[-1]))\n",
    "\n",
    "# Plot the function and gradient descent path\n",
    "x_vals = jnp.linspace(-1, 7, 100)\n",
    "y_vals = jnp.linspace(-8, 0, 100)\n",
    "X, Y   = jnp.meshgrid(x_vals, y_vals)\n",
    "Z      = f([X, Y])\n",
    "\n",
    "plt.contour(X, Y, Z, levels=20)\n",
    "plt.plot(Xs[:,0], Xs[:,1], '-o', color='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.gca().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Because we minimize $f(x,y)$, it can be seen as the loss function.\n",
    "Hence we can plot the evolution of the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(f(Xs.T))\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss f(x,y)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "To demonstrate a more complex optimization scenario, let's consider fitting a multi-parameter model to noisy data.\n",
    "We will use polynomial regression as our example, where we fit a polynomial curve to data points by optimizing the coefficients.\n",
    "This is a non-trivial problem because, as the degree of the polynomial increases, the number of parameters grows, resulting in a high-dimensional optimization task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth = np.array([1.2, -3, 0.5, 1.0, -1.8, 2.0, -0.1])\n",
    "\n",
    "Xdata = np.linspace(-1, 1, 1_000)\n",
    "Ytrue = sum(c * Xdata**i for i, c in enumerate(groundtruth))\n",
    "Ydata = Ytrue + np.random.normal(scale=0.1, size=Xdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Xdata, Ytrue)\n",
    "plt.plot(Xdata, Ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define polynomial model\n",
    "def model(Xs, Cs):\n",
    "    return sum(c * Xs**i for i, c in enumerate(Cs))\n",
    "\n",
    "# Define the objective function\n",
    "def chi2(Cs):\n",
    "    Ymodel = model(Xdata, Cs)\n",
    "    return jnp.mean((Ymodel - Ydata)**2)\n",
    "\n",
    "# Parameters for gradient descent\n",
    "C0    = jnp.zeros(len(groundtruth)) # Start with zeros as initial coefficients\n",
    "alpha = 0.1                         # Learning rate\n",
    "imax  = 1000                        # Number of iterations\n",
    "\n",
    "Cs = autogd_hist(chi2, C0, alpha, imax)\n",
    "%timeit -r1 Cs = autogd_hist(chi2, C0, alpha, imax)\n",
    "\n",
    "print(\"Optimized coefficients:\", Cs[-1])\n",
    "print(\"True coefficients:\",      groundtruth)\n",
    "print(\"Mean Squared Error:\",     np.mean((groundtruth - Cs[-1])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = 20\n",
    "plt.scatter(Xdata[::skip], Ydata[::skip], color='blue', label='Noisy Data', alpha=0.5)\n",
    "plt.plot(Xdata, Ytrue, 'g--', label='True Polynomial')\n",
    "for i, Ci in enumerate(Cs[::skip]):\n",
    "    Yfit = model(Xdata, Ci)\n",
    "    plt.plot(Xdata, Yfit, 'r', alpha=skip*i/imax, label='Fitted Polynomial' if skip*i == imax else '')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Let's also plot $\\chi^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Chi2 = [chi2(Ci) for Ci in Cs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(Chi2)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Chi2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
