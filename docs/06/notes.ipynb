{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Interpolation and Extrapolation\n",
    "\n",
    "This lecture follows closely [Numerical Recipes](https://numerical.recipes/) 2nd Edition in C and 3rd Edition in C++, Chapter 3 \"Interpolation and Extrapolation\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In scientific computing and machine learning, interpolation and extrapolation are fundamental tools for estimating function values at new data points based on known information.\n",
    "* In machine learning, all standard supervised learning tasks can be viewed as interpolation problems in high-dimensional space. Here, models predict outputs **within the range** of their training data.\n",
    "* However, when attempting to make predictions outside this range, we face significant challenges in making reliable extrapolations.\n",
    "  Extrapolation is a particularly challenging task because models typically lack information  beyond their training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Interpolation Methods\n",
    "\n",
    "Interpolation plays a crucial role in scientific computing and machine learning by estimating function values at new data points based on known information.\n",
    "\n",
    "* Polynomial interpolation is versatile but can exhibit significant oscillations, particularly at the edges of data (Runge's phenomenon).\n",
    "* This can be mitigated by using rational functions, which offer more stable estimates and are better suited to handle asymptotic behavior.\n",
    "* Spline interpolation, especially cubic splines, is valued for its smoothness and continuity up to the second derivative. This makes it effective for applications requiring a smooth fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Challenges with Extrapolation\n",
    "\n",
    "Extrapolation remains a difficult task, yet physics-informed machine learning (PIML) presents a promising avenue.\n",
    "By embedding known physical laws, such as ordinary differential equations (ODEs), into models, PIML enables extrapolation that aligns with fundamental constraints. This allows for meaningful extensions of predictions beyond the observed data range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Distinguishing Interpolation and Function Approximation\n",
    "\n",
    "Interpolation and function approximation are related but distinct tasks:\n",
    "\n",
    "* Interpolation estimates values at specified points within a given dataset.\n",
    "* In contrast, function approximation creates a simplified function to replace a more complex one.\n",
    "  This approach can be used to sample additional points as needed.\n",
    "* (See [Numerical Recipes](https://numerical.recipes/) Chapter 5 for function approximation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Limitations of Interpolation\n",
    "\n",
    "Even the most sophisticated interpolation schemes can fail when faced with pathological functions.\n",
    "For instance, consider a function that behaves smoothly except for a slight singularity at a certain point:\n",
    "\\begin{align}\n",
    "f(x) = 3x^2 + \\frac{1}{\\pi^4}\\ln\\left[(\\pi - x)^2\\right] + 1\n",
    "\\end{align}\n",
    "\n",
    "Interpolation based on values close to but not precisely at that singularity will likely produce an inaccurate result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return 3 * x**2 + np.log((np.pi - x)**2) / np.pi**4 + 1\n",
    "\n",
    "x1 = np.linspace(3.13, 3.16, 3+1)\n",
    "x2 = np.linspace(3.13, 3.16, 30+1)\n",
    "x3 = np.linspace(3.13, 3.16, 300+1)\n",
    "x4 = np.linspace(3.13, 3.16, 3000+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(x4, f(x4),       label='3001 points')\n",
    "plt.plot(x3, f(x3), '--', label='301 points')\n",
    "plt.plot(x2, f(x2), 'o:', label='31 points')\n",
    "plt.plot(x1, f(x1), 'o-', label='4 points')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "These cases highlight the importance of having some error estimates in interpolation routines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Preliminaries: Searching an Ordered Table\n",
    "\n",
    "In many interpolation tasks, especially with irregularly sampled data, the process begins with a critical first step: identifying the nearest points surrounding the target interpolation value.\n",
    "\n",
    "Unlike regularly spaced data on a uniform grid, where adjacent points are easy to locate by simple indexing, randomly sampled or unevenly spaced data requires additional steps to find nearby values.\n",
    "This searching step can be as computationally intensive as the interpolation itself, so efficient search methods are essential to maintain overall performance.\n",
    "\n",
    "In Numerical Recipes, two primary methods are presented for this purpose: bisection and hunting.\n",
    "Each is suited to different scenarios, depending on whether interpolation points tend to be close to one another or scattered randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Linear Search\n",
    "\n",
    "As a reference, we will implement a linear search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(xs, target):\n",
    "    for l in range(len(xs)): # purposely use for-loop to avoid C optimization in numpy\n",
    "        if xs[l] >= target:\n",
    "            return l-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for _ in range(10):\n",
    "    xs = np.sort(np.random.uniform(0, 100, 10))\n",
    "    v  = np.random.uniform(min(xs), max(xs))\n",
    "    i  = linear(xs, v)\n",
    "    print(f'{xs[i]} <= {v} < {xs[i+1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
